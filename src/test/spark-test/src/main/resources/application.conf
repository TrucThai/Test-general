####################################
# Apocalypse Reference Config    #
####################################


application {

  aggregatedDuration = "1m" # accept d(day), h(hour), m(minute)

  # alarmsink config to create redis
  servers = "192.168.1.131"
  port = 6379

  alarm{
    list = "alarm-list"
    keylist="alarm-key-list"
    pubsub="alarm-channel"
    condition{
      threshold{
        list = "threshold-list"
      }
    }
  }

  device{
    list = "device-list"
  }

  postgres{
    driver = "org.postgresql.Driver"
    connection = "jdbc:postgresql://192.168.1.69:5432/iot"
    user = "iot"
    pass = "123456"
  }

  cassandra {
    host = "192.168.1.131"
    port = 9042
    username=""
    password=""

    power {
      keyspace = "energy_data"
      table.raw_power_data_double = "raw_power_data_double"
      table.raw_power_data_long = "raw_power_data_long"
    }
  }

}

kafka-input {
  brokers = "192.168.1.131:9092" #[${?KAFKA_HOSTS}]
  topic.power= "temp-raw"
}

kafka-producer-druid {
  producer{
    bootstrap.servers = "192.168.1.131:9092" #[${?KAFKA_HOSTS}]
    acks="0"
    retries=0
    batch.size=16384
    linger.ms=0
    key.serializer=org.apache.kafka.common.serialization.StringSerializer
    value.serializer=org.apache.kafka.common.serialization.StringSerializer
  }

  topic.iot.double= "druid-iot"
  topic.iot.long= "druid-iot-long"
  topic.iot.alarm= "druid-iot-alarm"
}

redis {
  servers = "192.168.1.131"
  port = 6379
}

spark {
  master = "spark://192.168.1.131:7077"
  cleaner.ttl = 7200
  streaming.batch.interval = 30000
}

hadoop{
  hdfs="hdfs://192.168.1.73:9000",
  rawDevicePath="/DEVICE_TEMP",
  compressedDevicePath="/DEVICE_CMPRSS"
}
